{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81173011-b9be-4aae-8765-0052b7c564e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# modeling & tuning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# explainability\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from pdpbox import pdp, info_plots\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# set path to your dataset csv\n",
    "data_path = \"high_dim_financial_dataset.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "print(\"shape:\", df.shape)\n",
    "print(df.columns.tolist())\n",
    "print(df['date'].min(), df['date'].max())\n",
    "\n",
    "# compute 5-day forward return and direction\n",
    "df['return_5d'] = df['close'].pct_change(periods=5).shift(-5)  # forward 5-day return\n",
    "df['target'] = (df['return_5d'] > 0).astype(int)  # 1 if price up in 5 days else 0\n",
    "\n",
    "# drop rows where target is NaN (end of series)\n",
    "df = df.dropna(subset=['target']).reset_index(drop=True)\n",
    "\n",
    "def add_lag_rolling_features(df, cols, lags=[1,2,3,5,10], windows=[3,5,10,21]):\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            df[f\"{c}_lag{l}\"] = df[c].shift(l)\n",
    "        for w in windows:\n",
    "            df[f\"{c}_rollmean{w}\"] = df[c].rolling(window=w).mean()\n",
    "            df[f\"{c}_rollstd{w}\"] = df[c].rolling(window=w).std()\n",
    "            df[f\"{c}_rollmin{w}\"] = df[c].rolling(window=w).min()\n",
    "            df[f\"{c}_rollmax{w}\"] = df[c].rolling(window=w).max()\n",
    "    return df\n",
    "\n",
    "price_cols = ['close', 'volume']  # extend with other continuous columns you have\n",
    "df = add_lag_rolling_features(df, price_cols, lags=[1,2,3,5,10], windows=[3,5,10,21])\n",
    "\n",
    "# percent change features for stationarity (esp. macro series)\n",
    "for c in price_cols:\n",
    "    df[f\"{c}_pct_1\"] = df[c].pct_change(1)\n",
    "    df[f\"{c}_pct_5\"] = df[c].pct_change(5)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(\"After feature engineering:\", df.shape)\n",
    "\n",
    "#Task 1 end\n",
    "# exclude date, target, return_5d columns\n",
    "exclude = ['date', 'target', 'return_5d']\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "# remove near-constant columns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=1e-5)\n",
    "vt.fit(df[feature_cols])\n",
    "cols_nonconst = np.array(feature_cols)[vt.get_support()].tolist()\n",
    "\n",
    "print(f\"Kept {len(cols_nonconst)} non-constant features from {len(feature_cols)}\")\n",
    "feature_cols = cols_nonconst\n",
    "\n",
    "# correlation thresholding\n",
    "corr = df[feature_cols].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(\"Dropping correlated:\", len(to_drop))\n",
    "feature_cols = [c for c in feature_cols if c not in to_drop]\n",
    "\n",
    "#Task 2\n",
    "# simple time-based split: train 70%, valid 15%, test 15%\n",
    "n = len(df)\n",
    "train_end = int(0.7 * n)\n",
    "valid_end = int(0.85 * n)\n",
    "\n",
    "train_df = df.iloc[:train_end].copy()\n",
    "valid_df = df.iloc[train_end:valid_end].copy()\n",
    "test_df  = df.iloc[valid_end:].copy()\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['target']\n",
    "X_valid = valid_df[feature_cols]\n",
    "y_valid = valid_df['target']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['target']\n",
    "\n",
    "print(\"Train/Valid/Test sizes:\", X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "#Baseline: XGBoost pipeline + hyperparameter tuning (Task 2)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),   # scale continuous features\n",
    "    ('model', xgb_clf)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 300],\n",
    "    'model__max_depth': [3, 6],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gsearch.fit(X_train, y_train)\n",
    "print(\"Best params:\", gsearch.best_params_)\n",
    "print(\"Best CV AUC:\", gsearch.best_score_)\n",
    "\n",
    "#Evaluate on validation and test sets:\n",
    "best = gsearch.best_estimator_\n",
    "for name, X, y in [('VALID', X_valid, y_valid), ('TEST', X_test, y_test)]:\n",
    "    preds_proba = best.predict_proba(X)[:,1]\n",
    "    preds = best.predict(X)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(y, preds))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y, preds_proba))\n",
    "    print(classification_report(y, preds))\n",
    "    print(confusion_matrix(y, preds))\n",
    "\n",
    "#Model interpretability — SHAP (Task 3)\n",
    "# prepare explainer - if using pipeline, extract the trained XGB model for SHAP\n",
    "xgb_model = best.named_steps['model']\n",
    "scaler = best.named_steps['scaler']\n",
    "\n",
    "# We will explain standardized features (the model sees scaled features)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "# compute shap_values on a sample to save time\n",
    "sample_idx = np.random.choice(X_test_scaled.shape[0], size=min(2000, X_test_scaled.shape[0]), replace=False)\n",
    "shap_values_raw = explainer.shap_values(X_test_scaled[sample_idx])\n",
    "\n",
    "# Determine if shap_values_raw is a list (for classification) or a single array (sometimes for classification or regression)\n",
    "if isinstance(shap_values_raw, list):\n",
    "    # For binary classification, typically shap_values_raw is a list of two arrays: [shap_values_class_0, shap_values_class_1]\n",
    "    shap_values_positive_class = shap_values_raw[1]\n",
    "    expected_value_positive_class = explainer.expected_value[1]\n",
    "else:\n",
    "    # If it's a single array, assume it's for the positive class directly\n",
    "    shap_values_positive_class = shap_values_raw\n",
    "    expected_value_positive_class = explainer.expected_value # This should be a scalar in this case\n",
    "\n",
    "# convert to DataFrame with original column names\n",
    "shap_df = pd.DataFrame(shap_values_positive_class, columns=feature_cols)\n",
    "shap_abs_mean = np.abs(shap_df).mean().sort_values(ascending=False)\n",
    "top_shap = shap_abs_mean.head(30)\n",
    "print(\"Top SHAP features:\\n\", top_shap.head(20))\n",
    "\n",
    "shap.summary_plot(shap_values_positive_class, X_test_scaled[sample_idx], feature_names=feature_cols, plot_type=\"bar\")\n",
    "\n",
    "i_single_instance = 0  # choose the first sample from the sampled set for force plot\n",
    "# For force plot, we need a single instance's SHAP values and expected value\n",
    "shap.force_plot(\n",
    "    expected_value_positive_class,\n",
    "    shap_values_positive_class[i_single_instance],\n",
    "    X_test_scaled[sample_idx][i_single_instance],\n",
    "    feature_names=feature_cols,\n",
    "    matplotlib=True\n",
    ")\n",
    "# OR for Jupyter interactive:\n",
    "# shap.initjs(); shap.force_plot(expected_value_positive_class, shap_values_positive_class[i_single_instance], X_test_scaled[sample_idx][i_single_instance], feature_names=feature_cols)\n",
    "\n",
    "# LIME needs the data in original (unscaled) or scaled space depending on how you fit your model.\n",
    "# We'll create an explainer on scaled numpy arrays but we must pass a predict_proba wrapper.\n",
    "\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "\n",
    "# Create a wrapper to accept raw numpy -> apply scaler -> model predict_proba\n",
    "def predict_proba_raw(X_raw):\n",
    "    X_scaled = scaler.transform(X_raw)\n",
    "    return xgb_model.predict_proba(X_scaled)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    X_train_np,\n",
    "    feature_names=feature_cols,\n",
    "    class_names=['down','up'],\n",
    "    discretize_continuous=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# explain a test sample (raw)\n",
    "idx = 10  # test sample index within X_test\n",
    "exp = lime_explainer.explain_instance(X_test.iloc[idx].values, predict_proba_raw, num_features=10)\n",
    "print(exp.as_list())\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "# pick top features from SHAP\n",
    "import matplotlib.pyplot as plt\n",
    "from pdpbox import pdp # Import the pdp module\n",
    "\n",
    "top_features = list(top_shap.index[:5])\n",
    "\n",
    "for feat in top_features:\n",
    "    # Use pdp.pdp_isolate and pdp.pdp_plot\n",
    "    try:\n",
    "        pdp_go = pdp.pdp_isolate(model=xgb_model,\n",
    "                                 dataset=pd.DataFrame(scaler.transform(X_test), columns=feature_cols),\n",
    "                                 model_features=feature_cols,\n",
    "                                 feature=feat)\n",
    "        # Use pdp_plot directly after importing\n",
    "        pdp.pdp_plot(pdp_go, feat)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"PDP failed for\", feat, e)\n",
    "\n",
    "#Generate three data-driven investment hypotheses (Task 4)\n",
    "# 1) Identify top features and their mean SHAP sign (positive or negative impact)\n",
    "shap_mean = shap_df.mean().sort_values(key=lambda s: np.abs(s), ascending=False)\n",
    "top_feats = shap_mean.head(10)\n",
    "\n",
    "hypotheses = []\n",
    "for feat in top_feats.index[:6]:\n",
    "    mean_shap = shap_df[feat].mean()\n",
    "    direction = \"increases\" if mean_shap > 0 else \"decreases\"\n",
    "    magnitude = mean_shap\n",
    "    # compute change in model probability between 25th and 75th percentile of the feature using PDP\n",
    "    feat_vals = pd.DataFrame(scaler.transform(X_test), columns=feature_cols)[feat]\n",
    "    q25, q75 = np.percentile(feat_vals, [25,75])\n",
    "    try:\n",
    "        pdp_go = pdp.pdp_isolate(model=xgb_model,\n",
    "                                 dataset=pd.DataFrame(scaler.transform(X_test), columns=feature_cols),\n",
    "                                 model_features=feature_cols,\n",
    "                                 feature=feat)\n",
    "        # estimate expected probability change\n",
    "        prob_q25 = np.interp(q25, pdp_go.feature_grids, pdp_go.pdp)\n",
    "        prob_q75 = np.interp(q75, pdp_go.feature_grids, pdp_go.pdp)\n",
    "        delta = prob_q75 - prob_q25\n",
    "    except Exception:\n",
    "        # fallback: use shap mean sign only\n",
    "        delta = np.nan\n",
    "\n",
    "    hypotheses.append({\n",
    "        \"feature\": feat,\n",
    "        \"mean_shap\": mean_shap,\n",
    "        \"direction\": direction,\n",
    "        \"pdp_delta\": delta\n",
    "    })\n",
    "\n",
    "hypotheses_df = pd.DataFrame(hypotheses)\n",
    "hypotheses_df.head(6)\n",
    "\n",
    "\n",
    "def mk_statement(row):\n",
    "    feat = row['feature']\n",
    "    dir_word = \"higher\" if row['mean_shap'] > 0 else \"lower\"\n",
    "    pdp_text = (\" estimated PDP probability change {:.4f}\".format(row['pdp_delta'])\n",
    "                if not np.isnan(row['pdp_delta']) else \"\")\n",
    "    return f\"Hypothesis: When {feat} is {dir_word}, the model predicts a higher chance of 5-day price increase — SHAP mean={row['mean_shap']:.4f}.{pdp_text}\"\n",
    "\n",
    "for r in hypotheses_df.head(3).to_dict('records'):\n",
    "    print(mk_statement(r))\n",
    "\n",
    "\n",
    "# Reinstall pdpbox to resolve potential installation issues\n",
    "!pip uninstall pdpbox -y\n",
    "!pip install pdpbox\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append(\"Project: Interpretable ML for High-Dimensional Financial Time Series\\n\")\n",
    "report_lines.append(\"Methodology:\\n1) Feature engineering: lags, rolling stats, pct changes\\n2) Model: XGBoost time-series-aware CV and grid search\\n3) Interpretability: SHAP, LIME, PDP\\n\")\n",
    "report_lines.append(\"\\nModel performance (VALID):\\n\")\n",
    "# insert printed metrics\n",
    "val_preds = best.predict(X_valid)\n",
    "val_proba  = best.predict_proba(X_valid)[:,1]\n",
    "report_lines.append(classification_report(y_valid, val_preds))\n",
    "report_lines.append(\"\\nTop SHAP features:\\n\")\n",
    "report_lines.extend([f\"{f}: {v:.6f}\\n\" for f,v in top_shap.items()])\n",
    "\n",
    "# add hypotheses text\n",
    "report_lines.append(\"\\nInvestment Hypotheses:\\n\")\n",
    "for r in hypotheses_df.head(3).to_dict('records'):\n",
    "    report_lines.append(mk_statement(r) + \"\\n\")\n",
    "\n",
    "with open(\"submission_report.txt\", \"w\") as f:\n",
    "    f.writelines(report_lines)\n",
    "print(\"Saved submission_report.txt and saved model at models/xgb_ts_best.pkl\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "top_shap.head(20).plot.barh()\n",
    "plt.title(\"Top SHAP mean(|value|)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_shap_bar.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
